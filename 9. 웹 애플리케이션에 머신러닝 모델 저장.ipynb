{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 학습된 사이킷런 추정기 저장\n",
    "\n",
    "<br>\n",
    "<b>pickle 모듈</b>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import pickle\r\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#movieclassifier 디렉터리 안에 pkl_objects 서브디렉터리 만들어 직렬화된 파이썬 객체 저장\r\n",
    "dest = os.path.join('movieclassifier','pkl_objects')\r\n",
    "if not os.path.exists(dest):\r\n",
    "    os.makedirs(dest)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import nltk\r\n",
    "nltk.download('stopwords')#179개 불용어\r\n",
    "from nltk.corpus import stopwords#불용어\r\n",
    "stop = stopwords.words('english')\r\n",
    "pickle.dump(stop,\r\n",
    "           open(os.path.join(dest, 'stopwords.pkl'),'wb'),#wb매개변수 활용하여 이진모드로 파일열기\r\n",
    "           protocol = 4)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#로지스틱회귀 모델 피팅하고 저장해야해서 이전 단원 코드 복붙\r\n",
    "import pandas as pd\r\n",
    "#그냥 깃헙에서 가져옴\r\n",
    "df = pd.read_csv('C:/Users/LG/Desktop/2021/여름방학/MachineLearningTextbook/movie_data.csv',encoding='utf-8')\r\n",
    "df.head()\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import numpy as np\r\n",
    "import re#정규표현식(regular expression)\r\n",
    "def get_minibatch(doc_stream,size):#문서를 읽어 지정한 만큼 문서를 반환\r\n",
    "    docs, y = [],[]\r\n",
    "    try:\r\n",
    "        for _ in range(size):#지정한 size만큼\r\n",
    "            text, label = next(doc_stream)\r\n",
    "            docs.append(text)\r\n",
    "            y.append(label)\r\n",
    "    except StopIteration:\r\n",
    "        pass\r\n",
    "    return docs, y\r\n",
    "def stream_docs(path):#문서 하나씩 읽어서 반환되는 제너레이터 함수\r\n",
    "    with open(path,'r',encoding = 'utf-8') as csv:\r\n",
    "        next(csv)#헤더 넘기기\r\n",
    "        for line in csv:\r\n",
    "            text, label = line[:-3], int(line[-2])\r\n",
    "            yield text,label\r\n",
    "def tokenizer(text):\r\n",
    "    #텍스트 정제->불용어 제거-> 토큰으로 분리\r\n",
    "    text = re.sub('<[^>]*>','',text)#정규표현식 사용\r\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\r\n",
    "    text = (re.sub('[\\W]+',' ',text.lower()) +#단어가 아닌 문자 모두 제거\r\n",
    "            ''.join(emoticons).replace('-',''))#소문자로 바꿈, 이모티콘 변수를 처리 완료된 문자열 끝에 추가\r\n",
    "    tokenized = [w for w in text.split() if w not in stop]\r\n",
    "    return tokenized\r\n",
    "def preprocessor(text):#텍스트 데이터 정제\r\n",
    "    text = re.sub('<[^>]*>','',text)#정규표현식 사용\r\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\r\n",
    "    text = (re.sub('[\\W]+',' ',text.lower()) +#단어가 아닌 문자 모두 제거\r\n",
    "            ''.join(emoticons).replace('-',''))#소문자로 바꿈, 이모티콘 변수를 처리 완료된 문자열 끝에 추가\r\n",
    "    return text\r\n",
    "\r\n",
    "df['review'] = df['review'].apply(preprocessor)\r\n",
    "\r\n",
    "X_train = df.loc[:25000, 'review'].values\r\n",
    "y_train = df.loc[:25000, 'sentiment'].values\r\n",
    "X_test = df.loc[25000:,'review'].values\r\n",
    "y_test=df.loc[25000:,'sentiment'].values\r\n",
    "from sklearn.feature_extraction.text import HashingVectorizer#데이터 종류에 상관없이 사용 가능\r\n",
    "from sklearn.linear_model import SGDClassifier\r\n",
    "vect = HashingVectorizer(decode_error = 'ignore',\r\n",
    "                        n_features = 2**21,\r\n",
    "                        preprocessor = None,\r\n",
    "                        tokenizer=tokenizer)#해싱트릭사용\r\n",
    "clf = SGDClassifier(loss = 'log',random_state = 1, max_iter =1)#로지스틱 회귀 모델(loss = 'log')\r\n",
    "doc_stream = stream_docs(path = 'C:/Users/LG/Desktop/2021/여름방학/MachineLearningTextbook/movie_data.csv')\r\n",
    "\r\n",
    "import pyprind\r\n",
    "pbar = pyprind.ProgBar(45)#진행 막대\r\n",
    "classes = np.array([0,1])\r\n",
    "for _ in range(45):#45개 미니 배치\r\n",
    "    X_train, y_train = get_minibatch(doc_stream, size = 1000)#1000개 문서로 구성된 각 미니 배치\r\n",
    "    if not X_train:\r\n",
    "        break\r\n",
    "    X_train = vect.transform(X_train)\r\n",
    "    clf.partial_fit(X_train,y_train,classes = classes)\r\n",
    "    pbar.update()\r\n",
    "    \r\n",
    "X_test,y_test = get_minibatch(doc_stream,size=5000)#마지막 5000개 문서->평가\r\n",
    "X_test = vect.transform(X_test)\r\n",
    "clf = clf.partial_fit(X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:37\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "pickle.dump(clf,\r\n",
    "            open(os.path.join(dest, 'classifier.pkl'),'wb'),#classifier.pkl이라고 로지스틱 회귀 모델 저장\r\n",
    "           protocol=4)#로지스틱 회귀 직렬화하여 저장\r\n",
    "           "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Hashing Vectorizer 객체로 임포트할 수 있도록 파이썬 스크립트 작성(vectorizer.py)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "raw",
   "source": [
    "#vectorizer.py 내용\r\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\r\n",
    "import re\r\n",
    "import os\r\n",
    "import pickle\r\n",
    "\r\n",
    "cur_dir = os.path.dirname(__file__)\r\n",
    "stop = pickle.load(open(\r\n",
    "    os.path.join(cur_dir,\r\n",
    "                'pkl_objects',\r\n",
    "                'stopwords.pkl'),'rb'\r\n",
    "))\r\n",
    "\r\n",
    "\r\n",
    "def tokenizer(text):\r\n",
    "    #텍스트 정제->불용어 제거-> 토큰으로 분리\r\n",
    "    text = re.sub('<[^>]*>','',text)#정규표현식 사용\r\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\r\n",
    "    text = (re.sub('[\\W]+',' ',text.lower()) +#단어가 아닌 문자 모두 제거\r\n",
    "            ''.join(emoticons).replace('-',''))#소문자로 바꿈, 이모티콘 변수를 처리 완료된 문자열 끝에 추가\r\n",
    "    tokenized = [w for w in text.split() if w not in stop]\r\n",
    "    return tokenized\r\n",
    "\r\n",
    "vect = HashingVectorizer(decode_error = 'ignore',\r\n",
    "                        n_features = 2**21,\r\n",
    "                        preprocessor = None,\r\n",
    "                        tokenizer=tokenizer\r\n",
    "                        )"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#vectorizer를 임포트하고 분류기 복원\r\n",
    "import pickle\r\n",
    "import re\r\n",
    "import os\r\n",
    "from vectorizer import vect#from vectorizer.pyy라고 하면 안 불려와짐\r\n",
    "\r\n",
    "clf = pickle.load(open(\r\n",
    "    os.path.join('pkl_objects','classifier.pkl'),'rb'\r\n",
    "))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\r\n",
    "cwd = os.getcwd()\r\n",
    "cwd#현재 디렉토리 출력(디렉토리 옮김)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\LG\\\\Desktop\\\\2021\\\\여름방학\\\\MachineLearningTextbook\\\\movieclassifier'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "label = {0:'음성',1:'양성'}#반환된 정수 값을 텍스트 레이블로 매핑하기 위한 딕셔너리\r\n",
    "\r\n",
    "example = ['I love this movie']\r\n",
    "X = vect.transform(example)\r\n",
    "print(\"예측: %s\\n확률: %.2f%%\"%\r\n",
    "      (label[clf.predict(X)[0]],\r\n",
    "      np.max(clf.predict_proba(X))))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "예측: 양성\n",
      "확률: 0.81%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 데이터를 저장하기 위해 SQLite 데이터베이스 설정"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "movieclassifier 디렉터리 안에 개로운 SQLite 데이터베이스를 생성하고 두 개의 영화리뷰 저장"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import sqlite3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "conn = sqlite3.connect('reviews.sqlite')#SQLite 데이터베이스 파일에 연결\r\n",
    "c= conn.cursor()#커서를 통해 데이터베이스 레코드 조작\r\n",
    "c.execute('DROP TABLE IF EXISTS review_db')\r\n",
    "c.execute('CREATE TABLE review_db'#review_db 테이블과\r\n",
    "         '(review TEXT, sentiment INTEGER, date TEXT)')#컬럼 3개(review, sentiment, date) 생성\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x199b99e9f80>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "example1 = 'I love this movie'\r\n",
    "c.execute(\"INSERT INTO review_db\"\r\n",
    "         \"(review, sentiment, date) VALUES\"\r\n",
    "         \"(?, ?, DATETIME('now'))\", (example1,1))#DATETIME(now): 날짜와 시간 추가"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x199b99e9f80>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "example2 = 'I disliked this movie'\r\n",
    "c.execute(\"INSERT INTO review_db\"\r\n",
    "         \"(review, sentiment, date) VALUES\"\r\n",
    "         \"(?, ?, DATETIME('now'))\",#?,?: 영화리뷰텍스트(example1,example2)와 클래스 레이블(1,0)로 이루어진 튜플의 원소\r\n",
    "         (example2,0))\r\n",
    "conn.commit()#데이타베이스 변경사항 저장\r\n",
    "conn.close()#데이타베이스와 연결 닫음"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "conn = sqlite3.connect('reviews.sqlite')#다시 연결\r\n",
    "c = conn.cursor()\r\n",
    "c.execute(\"SELECT * FROM review_db WHERE date\"\r\n",
    "         \" BETWEEN '2017-01-01 00:00:00' AND DATETIME('now')\")#2017년부터 오늘까지 테이블에 추가된 모든 row 추출\r\n",
    "results = c.fetchall()#추출\r\n",
    "conn.close()#데이터베이스와 연결 닫음"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "print(results)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('I love this movie', 1, '2021-07-22 05:51:01'), ('I disliked this movie', 0, '2021-07-22 05:51:02')]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 플라스트 웹 애플리케이션 개발"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "!pip install flask"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting flask\n",
      "  Downloading Flask-2.0.1-py3-none-any.whl (94 kB)\n",
      "Requirement already satisfied: click>=7.1.2 in c:\\anaconda3\\envs\\dsfs\\lib\\site-packages (from flask) (8.0.1)\n",
      "Requirement already satisfied: Werkzeug>=2.0 in c:\\anaconda3\\envs\\dsfs\\lib\\site-packages (from flask) (2.0.1)\n",
      "Collecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\anaconda3\\envs\\dsfs\\lib\\site-packages (from flask) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\anaconda3\\envs\\dsfs\\lib\\site-packages (from click>=7.1.2->flask) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\anaconda3\\envs\\dsfs\\lib\\site-packages (from Jinja2>=3.0->flask) (2.0.1)\n",
      "Installing collected packages: itsdangerous, flask\n",
      "Successfully installed flask-2.0.1 itsdangerous-2.0.1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from flask import Flask, render_template"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}