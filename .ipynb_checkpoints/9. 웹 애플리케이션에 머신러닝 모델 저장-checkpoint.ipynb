{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87b03466",
   "metadata": {},
   "source": [
    "### 학습된 사이킷런 추정기 저장\n",
    "\n",
    "<br>\n",
    "<b>pickle 모듈</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8288088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "911f1e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#movieclassifier 디렉터리 안에 pkl_objects 서브디렉터리 만들어 직렬화된 파이썬 객체 저장\n",
    "dest = os.path.join('movieclassifier','pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e8dd63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')#179개 불용어\n",
    "from nltk.corpus import stopwords#불용어\n",
    "stop = stopwords.words('english')\n",
    "pickle.dump(stop,\n",
    "           open(os.path.join(dest, 'stopwords.pkl'),'wb'),#wb매개변수 활용하여 이진모드로 파일열기\n",
    "           protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75031ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#로지스틱회귀 모델 피팅하고 저장해야해서 이전 단원 코드 복붙\n",
    "import pandas as pd\n",
    "#그냥 깃헙에서 가져옴\n",
    "df = pd.read_csv('C:/Users/LG/Desktop/2021/여름방학/MachineLearningTextbook/movie_data.csv',encoding='utf-8')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb8d8f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:37\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re#정규표현식(regular expression)\n",
    "def get_minibatch(doc_stream,size):#문서를 읽어 지정한 만큼 문서를 반환\n",
    "    docs, y = [],[]\n",
    "    try:\n",
    "        for _ in range(size):#지정한 size만큼\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    return docs, y\n",
    "def stream_docs(path):#문서 하나씩 읽어서 반환되는 제너레이터 함수\n",
    "    with open(path,'r',encoding = 'utf-8') as csv:\n",
    "        next(csv)#헤더 넘기기\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text,label\n",
    "def tokenizer(text):\n",
    "    #텍스트 정제->불용어 제거-> 토큰으로 분리\n",
    "    text = re.sub('<[^>]*>','',text)#정규표현식 사용\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
    "    text = (re.sub('[\\W]+',' ',text.lower()) +#단어가 아닌 문자 모두 제거\n",
    "            ''.join(emoticons).replace('-',''))#소문자로 바꿈, 이모티콘 변수를 처리 완료된 문자열 끝에 추가\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "def preprocessor(text):#텍스트 데이터 정제\n",
    "    text = re.sub('<[^>]*>','',text)#정규표현식 사용\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
    "    text = (re.sub('[\\W]+',' ',text.lower()) +#단어가 아닌 문자 모두 제거\n",
    "            ''.join(emoticons).replace('-',''))#소문자로 바꿈, 이모티콘 변수를 처리 완료된 문자열 끝에 추가\n",
    "    return text\n",
    "\n",
    "df['review'] = df['review'].apply(preprocessor)\n",
    "\n",
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:,'review'].values\n",
    "y_test=df.loc[25000:,'sentiment'].values\n",
    "from sklearn.feature_extraction.text import HashingVectorizer#데이터 종류에 상관없이 사용 가능\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "vect = HashingVectorizer(decode_error = 'ignore',\n",
    "                        n_features = 2**21,\n",
    "                        preprocessor = None,\n",
    "                        tokenizer=tokenizer)#해싱트릭사용\n",
    "clf = SGDClassifier(loss = 'log',random_state = 1, max_iter =1)#로지스틱 회귀 모델(loss = 'log')\n",
    "doc_stream = stream_docs(path = 'C:/Users/LG/Desktop/2021/여름방학/MachineLearningTextbook/movie_data.csv')\n",
    "\n",
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)#진행 막대\n",
    "classes = np.array([0,1])\n",
    "for _ in range(45):#45개 미니 배치\n",
    "    X_train, y_train = get_minibatch(doc_stream, size = 1000)#1000개 문서로 구성된 각 미니 배치\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train,y_train,classes = classes)\n",
    "    pbar.update()\n",
    "    \n",
    "X_test,y_test = get_minibatch(doc_stream,size=5000)#마지막 5000개 문서->평가\n",
    "X_test = vect.transform(X_test)\n",
    "clf = clf.partial_fit(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db61845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf,\n",
    "            open(os.path.join(dest, 'classifier.pkl'),'wb'),#classifier.pkl이라고 로지스틱 회귀 모델 저장\n",
    "           protocol=4)#로지스틱 회귀 직렬화하여 저장\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61a6a3",
   "metadata": {},
   "source": [
    "#### Hashing Vectorizer 객체로 임포트할 수 있도록 파이썬 스크립트 작성(vectorizer.py)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8408db6",
   "metadata": {},
   "source": [
    "#vectorizer.py 내용\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "cur_dir = os.path.dirname(__file__)\n",
    "stop = pickle.load(open(\n",
    "    os.path.join(cur_dir,\n",
    "                'pkl_objects',\n",
    "                'stopwords.pkl'),'rb'\n",
    "))\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    #텍스트 정제->불용어 제거-> 토큰으로 분리\n",
    "    text = re.sub('<[^>]*>','',text)#정규표현식 사용\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
    "    text = (re.sub('[\\W]+',' ',text.lower()) +#단어가 아닌 문자 모두 제거\n",
    "            ''.join(emoticons).replace('-',''))#소문자로 바꿈, 이모티콘 변수를 처리 완료된 문자열 끝에 추가\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "vect = HashingVectorizer(decode_error = 'ignore',\n",
    "                        n_features = 2**21,\n",
    "                        preprocessor = None,\n",
    "                        tokenizer=tokenizer\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b896484a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vectorizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0837090c58d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;31m#from vectorizer.pyy라고 하면 안 불려와짐\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m clf = pickle.load(open(\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'vectorizer'"
     ]
    }
   ],
   "source": [
    "#vectorizer를 임포트하고 분류기 복원\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from vectorizer import vect#from vectorizer.pyy라고 하면 안 불려와짐\n",
    "\n",
    "clf = pickle.load(open(\n",
    "    os.path.join('pkl_objects','classifier.pkl'),'rb'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e5426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd#현재 디렉토리 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "label = {0:'음성',1:'양성'}#반환된 정수 값을 텍스트 레이블로 매핑하기 위한 딕셔너리\n",
    "\n",
    "example = ['I love this movie']\n",
    "X = vect.transform(example)\n",
    "print(\"예측: %s\\n확률: %.2f%%\"%\n",
    "      (label[clf.predict(X)[0]],\n",
    "      np.max(clf.predict_proba(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee98d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a20dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55ee2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2970ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
