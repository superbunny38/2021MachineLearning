{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOGNs9ERP/ApaBOKE9fdIy/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/superbunny38/MachineLearning/blob/main/CS229MachineLearning/2022AndrewLectureNotes/Probset2/Prob6/Set2Problem6(a)(b)(c)(d).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I557kkHGOAWt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import pandas as pd\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rights**<br>\n",
        "*Assignment made by Prof. Andrew Ng*<br>\n",
        "*Assignment solved by Chaeeun Ryu*"
      ],
      "metadata": {
        "id": "i_rxVwcQOHjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "ZEIpmq7gPppC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "\n",
        "def add_intercept_fn(x):\n",
        "    \"\"\"Add intercept to matrix x.\n",
        "\n",
        "    Args:\n",
        "        x: 2D NumPy array.\n",
        "\n",
        "    Returns:\n",
        "        New matrix same as x with 1's in the 0th column.\n",
        "    \"\"\"\n",
        "    new_x = np.zeros((x.shape[0], x.shape[1] + 1), dtype=x.dtype)\n",
        "    new_x[:, 0] = 1\n",
        "    new_x[:, 1:] = x\n",
        "\n",
        "    return new_x\n",
        "\n",
        "def load_csv(csv_path, label_col='y', add_intercept=False):\n",
        "    \"\"\"Load dataset from a CSV file.\n",
        "\n",
        "    Args:\n",
        "         csv_path: Path to CSV file containing dataset.\n",
        "         label_col: Name of column to use as labels (should be 'y' or 'l').\n",
        "         add_intercept: Add an intercept entry to x-values.\n",
        "\n",
        "    Returns:\n",
        "        xs: Numpy array of x-values (inputs).\n",
        "        ys: Numpy array of y-values (labels).\n",
        "    \"\"\"\n",
        "\n",
        "    # Load headers\n",
        "    with open(csv_path, 'r', newline='') as csv_fh:\n",
        "        headers = csv_fh.readline().strip().split(',')\n",
        "\n",
        "    # Load features and labels\n",
        "    x_cols = [i for i in range(len(headers)) if headers[i].startswith('x')]\n",
        "    l_cols = [i for i in range(len(headers)) if headers[i] == label_col]\n",
        "    inputs = np.loadtxt(csv_path, delimiter=',', skiprows=1, usecols=x_cols)\n",
        "    labels = np.loadtxt(csv_path, delimiter=',', skiprows=1, usecols=l_cols)\n",
        "\n",
        "    if inputs.ndim == 1:\n",
        "        inputs = np.expand_dims(inputs, -1)\n",
        "\n",
        "    if add_intercept:\n",
        "        inputs = add_intercept_fn(inputs)\n",
        "\n",
        "    return inputs, labels\n",
        "\n",
        "def load_spam_dataset(tsv_path):\n",
        "    \"\"\"Load the spam dataset from a TSV file\n",
        "\n",
        "    Args:\n",
        "         csv_path: Path to TSV file containing dataset.\n",
        "\n",
        "    Returns:\n",
        "        messages: A list of string values containing the text of each message.\n",
        "        labels: The binary labels (0 or 1) for each message. A 1 indicates spam.\n",
        "    \"\"\"\n",
        "\n",
        "    messages = []\n",
        "    labels = []\n",
        "\n",
        "    with open(tsv_path, 'r', newline='', encoding='utf8') as tsv_file:\n",
        "        reader = csv.reader(tsv_file, delimiter='\\t')\n",
        "\n",
        "        for label, message in reader:\n",
        "            messages.append(message)\n",
        "            labels.append(1 if label == 'spam' else 0)\n",
        "\n",
        "    return messages, np.array(labels)\n",
        "\n",
        "def plot(x, y, theta, save_path, correction=1.0):\n",
        "    \"\"\"Plot dataset and fitted logistic regression parameters.\n",
        "\n",
        "    Args:\n",
        "        x: Matrix of training examples, one per row.\n",
        "        y: Vector of labels in {0, 1}.\n",
        "        theta: Vector of parameters for logistic regression model.\n",
        "        save_path: Path to save the plot.\n",
        "        correction: Correction factor to apply (Problem 2(e) only).\n",
        "    \"\"\"\n",
        "    # Plot dataset\n",
        "    plt.figure()\n",
        "    plt.plot(x[y == 1, -2], x[y == 1, -1], 'bx', linewidth=2)\n",
        "    plt.plot(x[y == 0, -2], x[y == 0, -1], 'go', linewidth=2)\n",
        "\n",
        "    # Plot decision boundary (found by solving for theta^T x = 0)\n",
        "    x1 = np.arange(min(x[:, -2]), max(x[:, -2]), 0.01)\n",
        "    x2 = -(theta[0] / theta[2] * correction + theta[1] / theta[2] * x1)\n",
        "    plt.plot(x1, x2, c='red', linewidth=2)\n",
        "\n",
        "    # Add labels and save to disk\n",
        "    plt.xlabel('x1')\n",
        "    plt.ylabel('x2')\n",
        "    plt.savefig(save_path)\n",
        "\n",
        "\n",
        "def plot_contour(predict_fn):\n",
        "    \"\"\"Plot a contour given the provided prediction function\"\"\"\n",
        "    x, y = np.meshgrid(np.linspace(-10, 10, num=20), np.linspace(-10, 10, num=20))\n",
        "    z = np.zeros(x.shape)\n",
        "\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(y.shape[1]):\n",
        "            z[i, j] = predict_fn([x[i, j], y[i, j]])\n",
        "\n",
        "    plt.contourf(x, y, z, levels=[-float('inf'), 0, float('inf')], colors=['orange', 'cyan'])\n",
        "\n",
        "def plot_points(x, y):\n",
        "    \"\"\"Plot some points where x are the coordinates and y is the label\"\"\"\n",
        "    x_one = x[y == 0, :]\n",
        "    x_two = x[y == 1, :]\n",
        "    \n",
        "    plt.scatter(x_one[:,0], x_one[:,1], marker='x', color='red')\n",
        "    plt.scatter(x_two[:,0], x_two[:,1], marker='o', color='blue')\n",
        "\n",
        "def write_json(filename, value):\n",
        "    \"\"\"Write the provided value as JSON to the given filename\"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(value, f)"
      ],
      "metadata": {
        "id": "LpYhSzxMPrfa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spam classification"
      ],
      "metadata": {
        "id": "AaGoFWYAOS_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#svm\n",
        "# Important note: you do not have to modify this file for your homework.\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(123)\n",
        "\n",
        "\n",
        "def train_and_predict_svm(train_matrix, train_labels, test_matrix, radius):\n",
        "    \"\"\"Train an SVM model and predict the resulting labels on a test set.\n",
        "\n",
        "    Args: \n",
        "        train_matrix: A numpy array containing the word counts for the train set\n",
        "        train_labels: A numpy array containing the spam or not spam labels for the train set\n",
        "        test_matrix: A numpy array containing the word counts for the test set\n",
        "        radius: The RBF kernel radius to use for the SVM\n",
        "\n",
        "    Return: \n",
        "        The predicted labels for each message\n",
        "    \"\"\"\n",
        "    model = svm_train(train_matrix, train_labels, radius)\n",
        "    return svm_predict(model, test_matrix, radius)\n",
        "\n",
        "\n",
        "def svm_train(matrix, category, radius):\n",
        "    state = {}\n",
        "    M, N = matrix.shape\n",
        "    Y = 2 * category - 1\n",
        "    matrix = 1. * (matrix > 0)\n",
        "    squared = np.sum(matrix * matrix, axis=1)\n",
        "    gram = matrix.dot(matrix.T)\n",
        "    K = np.exp(-(squared.reshape((1, -1)) + squared.reshape((-1, 1)) - 2 * gram) / (2 * (radius ** 2)))\n",
        "\n",
        "    alpha = np.zeros(M)\n",
        "    alpha_avg = np.zeros(M)\n",
        "    L = 1. / (64 * M)\n",
        "    outer_loops = 10\n",
        "\n",
        "    alpha_avg = 0\n",
        "    ii = 0\n",
        "    while ii < outer_loops * M:\n",
        "        i = int(np.random.rand() * M)\n",
        "        margin = Y[i] * np.dot(K[i, :], alpha)\n",
        "        grad = M * L * K[:, i] * alpha[i]\n",
        "        if margin < 1:\n",
        "            grad -= Y[i] * K[:, i]\n",
        "        alpha -= grad / np.sqrt(ii + 1)\n",
        "        alpha_avg += alpha\n",
        "        ii += 1\n",
        "\n",
        "    alpha_avg /= (ii + 1) * M\n",
        "\n",
        "    state['alpha'] = alpha\n",
        "    state['alpha_avg'] = alpha_avg\n",
        "    state['Xtrain'] = matrix\n",
        "    state['Sqtrain'] = squared\n",
        "    return state\n",
        "\n",
        "\n",
        "def svm_predict(state, matrix, radius):\n",
        "    M, N = matrix.shape\n",
        "\n",
        "    Xtrain = state['Xtrain']\n",
        "    Sqtrain = state['Sqtrain']\n",
        "    matrix = 1. * (matrix > 0)\n",
        "    squared = np.sum(matrix * matrix, axis=1)\n",
        "    gram = matrix.dot(Xtrain.T)\n",
        "    K = np.exp(-(squared.reshape((-1, 1)) + Sqtrain.reshape((1, -1)) - 2 * gram) / (2 * (radius ** 2)))\n",
        "    alpha_avg = state['alpha_avg']\n",
        "    preds = K.dot(alpha_avg)\n",
        "    output = (1 + np.sign(preds)) // 2\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "H0gLgu_UPxJS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (a)\n",
        "Implement code for processing the spam messages into numpy arrays that can be fed into machine learning models. The provided code will then run your functions and save the resulting dictionary into `p06_dictionary` and a sample of the resulting training matrix into `p06_sample_train_matrix`"
      ],
      "metadata": {
        "id": "Ef1X_YCnOd6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "tdrtWfNqOc3j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir output"
      ],
      "metadata": {
        "id": "kbIsmiGbS4XN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (b)\n",
        "Implement a naive Bayes classifier for spam classification with multimodal event model and Laplace smoothing. Find a way to compute Naive Bayes' predicted class labels without explicitly representing very small numbers such as $p(x|y)$"
      ],
      "metadata": {
        "id": "F3dCU9IDVljf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (c)\n",
        "Intuitively, some tokens may be particularly indicative of an SMS being in a particular class. We can try to get an informal sense of how indicative token i is for the SPAM class by looking at:\n",
        "\n",
        "$$log\\frac{p(x_j = i|y=1)}{p(x_j = i|y=0)} = log(\\frac{P(token_i|email is SPAM)}{P(token_i email is NOTSPAM)})$$\n",
        "\n",
        "Complete the `get_top_five_naive_bayes_words` function within the provided code using the above formula in order to optain the 5 most indicative tokens."
      ],
      "metadata": {
        "id": "NEEwb-Ugh8Wq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (d)\n",
        "Support vector machines (SVMs) are an alternative machine learning model we discussed in class. We have provided you an SVM implementation (using a radical basis function (RBF) kernel). One important part of training an SVM parameterized by an RBF kernel is choosing an appropriate kernel radius. **Compute the best SVM radius which maximizes accuracy on the validation dataset.**"
      ],
      "metadata": {
        "id": "8vb3sdFsllkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.matrixlib import mat\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def get_words(message):\n",
        "    \"\"\"Get the normalized list of words from a message string.\n",
        "\n",
        "    This function should split a message into words, normalize them, and return\n",
        "    the resulting list. For splitting, you should split on spaces. For normalization,\n",
        "    you should convert everything to lowercase.\n",
        "\n",
        "    Args:\n",
        "        message: A string containing an SMS message\n",
        "\n",
        "    Returns:\n",
        "       The list of normalized words from the message.\n",
        "    \"\"\"\n",
        "\n",
        "    # *** START CODE HERE ***\n",
        "    #------splitting-----\n",
        "    splitted = message.split()\n",
        "    #------normalization-----\n",
        "    normalized = [x.lower() for x in splitted]\n",
        "    return normalized\n",
        "    # *** END CODE HERE ***\n",
        "\n",
        "\n",
        "def create_dictionary(messages):\n",
        "    \"\"\"Create a dictionary mapping words to integer indices.\n",
        "\n",
        "    This function should create a dictionary of word to indices using the provided\n",
        "    training messages. Use get_words to process each message. \n",
        "\n",
        "    Rare words are often not useful for modeling. Please only add words to the dictionary\n",
        "    if they occur in at least five messages.\n",
        "\n",
        "    Args:\n",
        "        messages: A list of strings containing SMS messages\n",
        "\n",
        "    Returns:\n",
        "        A python dict mapping words to integers.\n",
        "    \"\"\"\n",
        "    all_words = []\n",
        "    # *** START CODE HERE ***\n",
        "    for message in messages:\n",
        "      all_words += get_words(message)\n",
        "    count = 0\n",
        "    real_dict = {}\n",
        "    for key,value in collections.Counter(all_words).items():\n",
        "      if value >= 4:\n",
        "        real_dict[key] = count#0부터 채움\n",
        "        count += 1\n",
        "    return real_dict\n",
        "    # *** END CODE HERE ***\n",
        "\n",
        "'''\n",
        "apple|banana|car|duck|\n",
        "  2  |   0  | 1 | 0 \n",
        "'''\n",
        "def transform_text(messages, word_dictionary):\n",
        "    \"\"\"Transform a list of text messages into a numpy array for further processing.\n",
        "\n",
        "    This function should create a numpy array that contains the number of times each word\n",
        "    appears in each message. Each row in the resulting array should correspond to each \n",
        "    message and each column should correspond to a word.\n",
        "\n",
        "    Use the provided word dictionary to map words to column indices. Ignore words that \n",
        "    are not present in the dictionary. Use get_words to get the words for a message.\n",
        "\n",
        "    Args:\n",
        "        messages: A list of strings where each string is an SMS message.\n",
        "        word_dictionary: A python dict mapping words to integers.\n",
        "\n",
        "    Returns:\n",
        "        A numpy array marking the words present in each message.\n",
        "    \"\"\"\n",
        "    # *** START CODE HERE ***\n",
        "    matrix = []\n",
        "    for message in messages:\n",
        "      row = np.zeros(len(word_dictionary.keys()))\n",
        "      words_list = get_words(message)\n",
        "      words_freq = collections.Counter(words_list)\n",
        "      for word, freq in words_freq.items():\n",
        "        if word in word_dictionary.keys():\n",
        "          word_to_integer = word_dictionary[word]\n",
        "          row[word_to_integer] = freq\n",
        "      matrix.append(row)\n",
        "    return np.array(matrix)\n",
        "\n",
        "    # *** END CODE HERE ***\n",
        "\n",
        "\n",
        "def fit_naive_bayes_model(matrix, labels, dictionary):\n",
        "    \"\"\"Fit a naive bayes model.\n",
        "\n",
        "    This function should fit a Naive Bayes model given a training matrix and labels.\n",
        "\n",
        "    The function should return the state of that model.\n",
        "\n",
        "    Feel free to use whatever datatype you wish for the state of the model.\n",
        "\n",
        "    Args:\n",
        "        matrix: A numpy array containing word counts for the training data (n_row: n_samples, n_columns: n_words in dict)\n",
        "        labels: The binary (0 or 1) labels for that training data\n",
        "\n",
        "    Returns: The trained model\n",
        "    \"\"\"\n",
        "\n",
        "    # *** START CODE HERE ***\n",
        "    unique, counts = np.unique(labels, return_counts = True)\n",
        "    assert unique[0] == 0, print(unique[0])\n",
        "    assert unique[1] == 1, print(unique[1])\n",
        "    k = len(dictionary.keys())#laplace smoothing\n",
        "    p0 = counts[0]/np.sum(counts)\n",
        "    p1 = counts[1]/np.sum(counts)\n",
        "    phi0_dict = {}\n",
        "    phi1_dict = {}\n",
        "    for _ in range(len(matrix[0])):\n",
        "      phi0_dict[_] = 0\n",
        "      phi1_dict[_] = 0\n",
        "    for i, data_x in enumerate(matrix):\n",
        "      data_y = labels[i]\n",
        "      for idx, j in enumerate(data_x):#j: word idx 개수\n",
        "        if j!= 0:\n",
        "          if data_y == 1:\n",
        "            #update phi1_dict\n",
        "            phi1_dict[idx] += j\n",
        "          elif data_y == 0:\n",
        "            #update phi0_dict\n",
        "            phi0_dict[idx] += j\n",
        "    #add 1 for numerator according to laplace smoothing\n",
        "    for _ in range(len(matrix[0])):\n",
        "      phi0_dict[_] += 1\n",
        "      phi1_dict[_] += 1\n",
        "    #calculate denominator\n",
        "    denom_1 = k#|V|\n",
        "    denom_0 = k#|V|\n",
        "    for i, data_x in enumerate(matrix):\n",
        "      if labels[i] == 0:\n",
        "        denom_0 += np.sum(data_x)\n",
        "      elif labels[i] == 1:\n",
        "        denom_1 += np.sum(data_x)\n",
        "    #divide by denominator\n",
        "    for _ in range(len(matrix[0])):\n",
        "      phi0_dict[_] = phi0_dict[_]/denom_0\n",
        "      phi1_dict[_] = phi1_dict[_]/denom_1\n",
        "    print(f\"p0: {p0}, p1: {p1}\")\n",
        "    return {'p(y=0)':p0,'p(y=1)':p1, 'k':k, 'phi0_dict':phi0_dict, 'phi1_dict':phi1_dict}#model\n",
        "    # *** END CODE HERE ***\n",
        "\n",
        "\n",
        "def predict_from_naive_bayes_model(model, matrix):\n",
        "    \"\"\"Use a Naive Bayes model to compute predictions for a target matrix.\n",
        "\n",
        "    This function should be able to predict on the models that fit_naive_bayes_model\n",
        "    outputs.\n",
        "\n",
        "    Args:\n",
        "        model: A trained model from fit_naive_bayes_model\n",
        "        matrix: A numpy array containing word counts\n",
        "\n",
        "    Returns: A numpy array containg the predictions from the model\n",
        "    \"\"\"\n",
        "    \n",
        "    # *** START CODE HERE ***\n",
        "    p0 = model['p(y=0)']\n",
        "    p1 = model['p(y=1)']\n",
        "    k = model['k']\n",
        "    phi0_dict = model['phi0_dict']\n",
        "    phi1_dict = model['phi1_dict']\n",
        "    predictions = []\n",
        "    for test_x in matrix:\n",
        "      p0_ = p0\n",
        "      p1_ = p1\n",
        "      for word_idx, freq in enumerate(test_x):\n",
        "        p0_ = np.exp(p0_*phi0_dict[word_idx])\n",
        "        p1_ = np.exp(p1_*phi1_dict[word_idx])\n",
        "      if p0_ > p1_:\n",
        "        predictions.append(0)\n",
        "      elif p1_ > p0_:\n",
        "        predictions.append(1)\n",
        "      else:\n",
        "        print(\"underflow\")\n",
        "        return\n",
        "    assert len(predictions) == matrix.shape[0]\n",
        "    return np.array(predictions)\n",
        "    # *** END CODE HERE ***\n",
        "\n",
        "\n",
        "def get_top_five_naive_bayes_words(model, dictionary):#how indicative token i is for the SPAM class\n",
        "    \"\"\"Compute the top five words that are most indicative of the spam (i.e positive) class.\n",
        "\n",
        "    Ues the metric given in 6c as a measure of how indicative a word is.\n",
        "    Return the words in sorted form, with the most indicative word first.\n",
        "\n",
        "    Args:\n",
        "        model: The Naive Bayes model returned from fit_naive_bayes_model\n",
        "        dictionary: A mapping of word to integer ids\n",
        "\n",
        "    Returns: The top five most indicative words in sorted order with the most indicative first\n",
        "    \"\"\"\n",
        "    # *** START CODE HERE ***\n",
        "    indicator_dict = {}\n",
        "    phi0_dict = model['phi0_dict']\n",
        "    phi1_dict = model['phi1_dict']\n",
        "    for word_idx in phi0_dict.keys():\n",
        "      indicator_dict[word_idx] = np.log(phi1_dict[word_idx]/phi0_dict[word_idx])\n",
        "    \n",
        "    sorted_dict = dict( sorted(indicator_dict.items(),\n",
        "                           key=lambda item: item[1],\n",
        "                           reverse=True))\n",
        "    count = 0\n",
        "    top5_keys = []\n",
        "    for key, value in sorted_dict.items():\n",
        "      if count > 4:\n",
        "        break\n",
        "      top5_keys.append(key)\n",
        "      count+=1\n",
        "\n",
        "    top5_ = []\n",
        "    for word_idx in top5_keys:\n",
        "      for word, idx in dictionary.items():\n",
        "        if word_idx == idx:\n",
        "          top5_.append(word)\n",
        "    assert len(top5_) == 5\n",
        "    return top5_\n",
        "    # *** END CODE HERE ***\n",
        "\n",
        "\n",
        "def compute_best_svm_radius(train_matrix, train_labels, val_matrix, val_labels, radius_to_consider):\n",
        "    \"\"\"Compute the optimal SVM radius using the provided training and evaluation datasets.\n",
        "\n",
        "    You should only consider radius values within the radius_to_consider list.\n",
        "    You should use accuracy as a metric for comparing the different radius values.\n",
        "\n",
        "    Args:\n",
        "        train_matrix: The word counts for the training data\n",
        "        train_labels: The spma or not spam labels for the training data\n",
        "        val_matrix: The word counts for the validation data\n",
        "        val_labels: The spam or not spam labels for the validation data\n",
        "        radius_to_consider: The radius values to consider\n",
        "    \n",
        "    Returns:\n",
        "        The best radius which maximizes SVM accuracy.\n",
        "    \"\"\"\n",
        "    # *** START CODE HERE ***\n",
        "    best_val_acc = 0.\n",
        "    for radius in radius_to_consider:\n",
        "      val_preds = train_and_predict_svm(train_matrix, train_labels, val_matrix, radius)\n",
        "      val_acc = np.mean(val_preds == val_labels)\n",
        "      if val_acc > best_val_acc:\n",
        "        best_radius = radius\n",
        "        best_val_acc = val_acc\n",
        "    print(f\"best validation acc:{best_val_acc} (w/ radius {best_radius})\")\n",
        "    return best_radius\n",
        "    # *** END CODE HERE ***\n",
        "\n",
        "\n",
        "def main():\n",
        "    train_messages, train_labels = load_spam_dataset('./data/ds6_train.tsv')\n",
        "    val_messages, val_labels = load_spam_dataset('./data/ds6_val.tsv')\n",
        "    test_messages, test_labels = load_spam_dataset('./data/ds6_test.tsv')\n",
        "    print(\">> ... data loaded\")\n",
        "    dictionary = create_dictionary(train_messages)\n",
        "\n",
        "    write_json('./output/p06_dictionary', dictionary)\n",
        "\n",
        "    train_matrix = transform_text(train_messages, dictionary)\n",
        "    # print(\"train mat shape:\",train_matrix.shape)\n",
        "    np.savetxt('./output/p06_sample_train_matrix', train_matrix[:100,:])\n",
        "\n",
        "    val_matrix = transform_text(val_messages, dictionary)\n",
        "    test_matrix = transform_text(test_messages, dictionary)\n",
        "    print(\">> ... word embedding done\")\n",
        "\n",
        "    naive_bayes_model = fit_naive_bayes_model(train_matrix, train_labels, dictionary)\n",
        "    print(\">> ... fitting naive bayes model done\")\n",
        "\n",
        "    naive_bayes_predictions = predict_from_naive_bayes_model(naive_bayes_model, test_matrix)\n",
        "\n",
        "    np.savetxt('./output/p06_naive_bayes_predictions', naive_bayes_predictions)\n",
        "\n",
        "    naive_bayes_accuracy = np.mean(naive_bayes_predictions == test_labels)\n",
        "\n",
        "    print('Naive Bayes (w/ laplace smoothing) had an accuracy of {} on the testing set'.format(naive_bayes_accuracy))\n",
        "\n",
        "    top_5_words = get_top_five_naive_bayes_words(naive_bayes_model, dictionary)\n",
        "\n",
        "    print('The top 5 indicative words for Naive Bayes are: ', top_5_words)\n",
        "\n",
        "    write_json('./output/p06_top_indicative_words', top_5_words)\n",
        "\n",
        "    optimal_radius = compute_best_svm_radius(train_matrix, train_labels, val_matrix, val_labels, [0.01, 0.1, 1, 10])\n",
        "\n",
        "    write_json('./output/p06_optimal_radius', optimal_radius)\n",
        "\n",
        "    print('The optimal SVM radius was {}'.format(optimal_radius))\n",
        "\n",
        "    svm_predictions = train_and_predict_svm(train_matrix, train_labels, test_matrix, optimal_radius)\n",
        "\n",
        "    svm_accuracy = np.mean(svm_predictions == test_labels)\n",
        "\n",
        "    print('The SVM model had an accuracy of {} on the testing set'.format(svm_accuracy, optimal_radius))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69tbxfhqOHAU",
        "outputId": "6f561ca2-720c-4ae3-b48e-194b3a85f1b2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> ... data loaded\n",
            ">> ... word embedding done\n",
            "p0: 0.8629122728292573, p1: 0.13708772717074266\n",
            ">> ... fitting naive bayes model done\n",
            "Naive Bayes (w/ laplace smoothing) had an accuracy of 0.8799283154121864 on the testing set\n",
            "The top 5 indicative words for Naive Bayes are:  ['claim', 'won', 'prize', 'tone', 'urgent!']\n",
            "best validation acc:0.9443447037701975 (w/ radius 0.1)\n",
            "The optimal SVM radius was 0.1\n",
            "The SVM model had an accuracy of 0.967741935483871 on the testing set\n"
          ]
        }
      ]
    }
  ]
}